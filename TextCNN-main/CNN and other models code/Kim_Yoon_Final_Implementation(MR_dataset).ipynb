{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8364bd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "#print(os.listdir(\"C:/Users/input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e858018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import some stuff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a586ebb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_11216\\3657699696.py:4: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A series of escapades demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                         Phrase  \\\n",
       "0  A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .   \n",
       "1  A series of escapades demonstrating the adage that what is good for the goose                                                                                                                  \n",
       "2  A series                                                                                                                                                                                       \n",
       "\n",
       "   Sentiment  \n",
       "0  1          \n",
       "1  2          \n",
       "2  2          "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/HP/Downloads/train.tsv', delimiter='\\t')\n",
    "df = df[['Phrase', 'Sentiment']]\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe156c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 0], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aeb3e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(in_str):\n",
    "    in_str = str(in_str)\n",
    "    # replace urls with 'url'\n",
    "    in_str = re.sub(r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})\", \"url\", in_str)\n",
    "    in_str = re.sub(r'([^\\s\\w]|_)+', '', in_str)\n",
    "    return in_str.strip().lower()\n",
    "\n",
    "\n",
    "df['text'] = df['Phrase'].apply(clean_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a13a85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    79582\n",
       "3    32927\n",
       "1    27273\n",
       "4    9206 \n",
       "0    7072 \n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c8ee39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = df[df['Sentiment'] == 0].sample(frac=1)\n",
    "df_1 = df[df['Sentiment'] == 1].sample(frac=1)\n",
    "df_2 = df[df['Sentiment'] == 2].sample(frac=1)\n",
    "df_3 = df[df['Sentiment'] == 3].sample(frac=1)\n",
    "df_4 = df[df['Sentiment'] == 4].sample(frac=1)\n",
    "\n",
    "# we want a balanced set for training against - there are 7072 `0` examples\n",
    "sample_size = 7072\n",
    "\n",
    "data = pd.concat([df_0.head(sample_size), df_1.head(sample_size), df_2.head(sample_size), df_3.head(sample_size), df_4.head(sample_size)]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d8b557b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean length of sentence: 9.109756787330317\n",
      "max length of sentence: 52\n",
      "std dev length of sentence: 8.006182447173451\n"
     ]
    }
   ],
   "source": [
    "data['l'] = data['Phrase'].apply(lambda x: len(str(x).split(' ')))\n",
    "print(\"mean length of sentence: \" + str(data.l.mean()))\n",
    "print(\"max length of sentence: \" + str(data.l.max()))\n",
    "print(\"std dev length of sentence: \" + str(data.l.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8d8454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these sentences aren't that long so we may as well use the whole string\n",
    "sequence_length = 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f4683b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set size 3536\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 52\n",
    "max_features = 20000 # this is the number of words we care about\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>')\n",
    "tokenizer.fit_on_texts(data['Phrase'].values)\n",
    "\n",
    "# this takes our sentences and replaces each word with an integer\n",
    "X = tokenizer.texts_to_sequences(data['Phrase'].values)\n",
    "\n",
    "# we then pad the sequences so they're all the same length (sequence_length)\n",
    "X = pad_sequences(X, sequence_length)\n",
    "\n",
    "y = pd.get_dummies(data['Sentiment']).values\n",
    "\n",
    "# where there isn't a test set, Kim keeps back 10% of the data for testing, I'm going to do the same since we have an ok amount to play with\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "print(\"test set size \" + str(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a453d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d870c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "review = data['Phrase'].values\n",
    "label = data['Sentiment'].values\n",
    "review_train, review_test, label_train, label_test = train_test_split(review, label, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e89f9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<26520x13323 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 205253 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "review_vectorizer = CountVectorizer()\n",
    "review_vectorizer.fit(review_train)\n",
    "Xlr_train = review_vectorizer.transform(review_train)\n",
    "Xlr_test  = review_vectorizer.transform(review_test)\n",
    "Xlr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5010b70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5432126696832579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LRmodel = LogisticRegression()\n",
    "LRmodel.fit(Xlr_train, label_train)\n",
    "score = LRmodel.score(Xlr_test, label_test)\n",
    "print(\"Accuracy:\", score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94ae23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d61c7d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(Xlr_train, label_train)  #model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88b85de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.70      0.58      1761\n",
      "           1       0.41      0.38      0.40      1764\n",
      "           2       0.50      0.22      0.31      1765\n",
      "           3       0.39      0.33      0.36      1752\n",
      "           4       0.55      0.75      0.63      1798\n",
      "\n",
      "    accuracy                           0.48      8840\n",
      "   macro avg       0.47      0.48      0.46      8840\n",
      "weighted avg       0.47      0.48      0.46      8840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict = nb_tfidf.predict(Xlr_test)\n",
    "#y_prob = nb_tfidf.predict_proba(X_val_vectors_tfidf)[:,1]\n",
    " \n",
    "\n",
    "print(classification_report(label_test,y_predict))\n",
    "#print('Confusion Matrix:',confusion_matrix(y_val, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34232104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1: Random embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4737516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300 # Kim uses 300 here\n",
    "num_filters = 100\n",
    "\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "\n",
    "# use a random embedding for the text\n",
    "embedding_layer = Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "\n",
    "reshape = Reshape((sequence_length, embedding_dim, 1))(embedding_layer)\n",
    "\n",
    "# Note the relu activation which Kim specifically mentions\n",
    "# He also uses an l2 constraint of 3\n",
    "# Also, note that the convolution window acts on the whole 200 dimensions - that's important\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "\n",
    "# perform max pooling on each of the convoluations\n",
    "maxpool_0 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "# concat and flatten\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "\n",
    "# do dropout and predict\n",
    "dropout = Dropout(0.5)(flatten)\n",
    "output = Dense(units=5, activation='softmax')(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af9c669d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 52)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 52, 300)      6000000     ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 52, 300, 1)   0           ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 50, 1, 100)   90100       ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 49, 1, 100)   120100      ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 48, 1, 100)   150100      ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 3, 1, 100)    0           ['max_pooling2d_3[0][0]',        \n",
      "                                                                  'max_pooling2d_4[0][0]',        \n",
      "                                                                  'max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 300)          0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 300)          0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 5)            1505        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,361,805\n",
      "Trainable params: 6,361,805\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b15159c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "896/896 [==============================] - 80s 89ms/step - loss: 1.6991 - accuracy: 0.2386 - val_loss: 1.6376 - val_accuracy: 0.2806\n",
      "Epoch 2/10\n",
      "896/896 [==============================] - 95s 106ms/step - loss: 1.6188 - accuracy: 0.2893 - val_loss: 1.6294 - val_accuracy: 0.3305\n",
      "Epoch 3/10\n",
      "896/896 [==============================] - 81s 91ms/step - loss: 1.5862 - accuracy: 0.3261 - val_loss: 1.6816 - val_accuracy: 0.3277\n",
      "Epoch 4/10\n",
      "896/896 [==============================] - 79s 88ms/step - loss: 1.5510 - accuracy: 0.3543 - val_loss: 1.5516 - val_accuracy: 0.3704\n",
      "Epoch 5/10\n",
      "896/896 [==============================] - 79s 88ms/step - loss: 1.5190 - accuracy: 0.3744 - val_loss: 1.5256 - val_accuracy: 0.3789\n",
      "Epoch 6/10\n",
      "896/896 [==============================] - 80s 89ms/step - loss: 1.4941 - accuracy: 0.3881 - val_loss: 1.5327 - val_accuracy: 0.3541\n",
      "Epoch 7/10\n",
      "896/896 [==============================] - 79s 89ms/step - loss: 1.4777 - accuracy: 0.4017 - val_loss: 1.5520 - val_accuracy: 0.3629\n",
      "Epoch 8/10\n",
      "896/896 [==============================] - 79s 89ms/step - loss: 1.4703 - accuracy: 0.4154 - val_loss: 1.5600 - val_accuracy: 0.3924\n",
      "Epoch 9/10\n",
      "896/896 [==============================] - 79s 89ms/step - loss: 1.4567 - accuracy: 0.4201 - val_loss: 1.6068 - val_accuracy: 0.3889\n",
      "Epoch 10/10\n",
      "896/896 [==============================] - 79s 89ms/step - loss: 1.4500 - accuracy: 0.4327 - val_loss: 1.5909 - val_accuracy: 0.3764\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # Kim uses 50 here, I have a slightly smaller sample size than num\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=batch_size, verbose=1, validation_split=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b66c694",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43macc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acc'"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e254c0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 1s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[118, 162,  94, 159, 148],\n",
       "       [ 55, 186, 299, 110,  42],\n",
       "       [ 18, 110, 528,  41,  19],\n",
       "       [ 71, 163, 238, 112, 127],\n",
       "       [ 89, 104,  59, 110, 374]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = model.predict(X_test)\n",
    "accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))\n",
    "confusion_matrix(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aafecc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2: Static word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1fc4f3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1917494 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('C:/Users/HP/Downloads/', 'glove.42B.300d.txt'),encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67b593f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14114 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84c92c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14115\n"
     ]
    }
   ],
   "source": [
    "num_words = min(max_features, len(word_index)) + 1\n",
    "print(num_words)\n",
    "\n",
    "# first create a matrix of zeros, this is our embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# for each word in out tokenizer lets try to find that work in our w2v model\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # we found the word - add that words vector to the matrix\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # doesn't exist, assign a random vector\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73b8f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_2 = Input(shape=(sequence_length,), dtype='int32')\n",
    "\n",
    "# note the `trainable=False`, later we will make this layer trainable\n",
    "embedding_layer_2 = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=sequence_length,\n",
    "                            trainable=False)(inputs_2)\n",
    "\n",
    "reshape_2 = Reshape((sequence_length, embedding_dim, 1))(embedding_layer_2)\n",
    "\n",
    "conv_0_2 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)\n",
    "conv_1_2 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)\n",
    "conv_2_2 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)\n",
    "\n",
    "maxpool_0_2 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_2)\n",
    "maxpool_1_2 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_2)\n",
    "maxpool_2_2 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_2)\n",
    "\n",
    "concatenated_tensor_2 = Concatenate(axis=1)([maxpool_0_2, maxpool_1_2, maxpool_2_2])\n",
    "flatten_2 = Flatten()(concatenated_tensor_2)\n",
    "\n",
    "dropout_2 = Dropout(0.5)(flatten_2)\n",
    "output_2 = Dense(units=5, activation='softmax')(dropout_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca253759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 52)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 52, 300)      4234500     ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 52, 300, 1)   0           ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 50, 1, 100)   90100       ['reshape_3[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 49, 1, 100)   120100      ['reshape_3[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 48, 1, 100)   150100      ['reshape_3[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 3, 1, 100)    0           ['max_pooling2d_6[0][0]',        \n",
      "                                                                  'max_pooling2d_7[0][0]',        \n",
      "                                                                  'max_pooling2d_8[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 300)          0           ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 300)          0           ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 5)            1505        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,596,305\n",
      "Trainable params: 361,805\n",
      "Non-trainable params: 4,234,500\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_2 = Model(inputs=inputs_2, outputs=output_2)\n",
    "model_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad816519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "796/796 [==============================] - 26s 31ms/step - loss: 1.8465 - accuracy: 0.2768 - val_loss: 1.7531 - val_accuracy: 0.2812\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 25s 32ms/step - loss: 1.7494 - accuracy: 0.2835 - val_loss: 1.7192 - val_accuracy: 0.3007\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 25s 32ms/step - loss: 1.7233 - accuracy: 0.2865 - val_loss: 1.7073 - val_accuracy: 0.3271\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 25s 32ms/step - loss: 1.7044 - accuracy: 0.2972 - val_loss: 1.6898 - val_accuracy: 0.3904\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 26s 33ms/step - loss: 1.6923 - accuracy: 0.2990 - val_loss: 1.6601 - val_accuracy: 0.3626\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 25s 32ms/step - loss: 1.6677 - accuracy: 0.3085 - val_loss: 1.6661 - val_accuracy: 0.3128\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 26s 32ms/step - loss: 1.6593 - accuracy: 0.3208 - val_loss: 1.6482 - val_accuracy: 0.4019\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 26s 32ms/step - loss: 1.6565 - accuracy: 0.3319 - val_loss: 1.6272 - val_accuracy: 0.4003\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 26s 32ms/step - loss: 1.6450 - accuracy: 0.3428 - val_loss: 1.6169 - val_accuracy: 0.4297\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 26s 33ms/step - loss: 1.6394 - accuracy: 0.3474 - val_loss: 1.5935 - val_accuracy: 0.4262\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "history_2 = model_2.fit(X_train, y_train, epochs=10, batch_size=batch_size, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6f8ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_2.history['acc'])\n",
    "plt.plot(history_2.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_2.history['loss'])\n",
    "plt.plot(history_2.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05da96d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 1s 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[562,  28,  14,   5,  72],\n",
       "       [406,  75,  80,  11, 120],\n",
       "       [171,  45, 297,  38, 165],\n",
       "       [161,  27, 126,  36, 361],\n",
       "       [112,   6,  28,  16, 574]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_hat_2 = model_2.predict(X_test)\n",
    "accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_2)))\n",
    "confusion_matrix(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04329546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 3: w2v with trainable embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d576313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_3 = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding_layer_3 = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=sequence_length,\n",
    "                            trainable=True)(inputs_3)\n",
    "\n",
    "reshape_3 = Reshape((sequence_length, embedding_dim, 1))(embedding_layer_3)\n",
    "\n",
    "# note the relu activation\n",
    "conv_0_3 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\n",
    "conv_1_3 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\n",
    "conv_2_3 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\n",
    "\n",
    "maxpool_0_3 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_3)\n",
    "maxpool_1_3 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_3)\n",
    "maxpool_2_3 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_3)\n",
    "\n",
    "concatenated_tensor_3 = Concatenate(axis=1)([maxpool_0_3, maxpool_1_3, maxpool_2_3])\n",
    "flatten_3 = Flatten()(concatenated_tensor_3)\n",
    "\n",
    "dropout_3 = Dropout(0.5)(flatten_3)\n",
    "output_3 = Dense(units=5, activation='softmax')(dropout_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "477b38f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 52)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 52, 300)      4234500     ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)            (None, 52, 300, 1)   0           ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 50, 1, 100)   90100       ['reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 49, 1, 100)   120100      ['reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 48, 1, 100)   150100      ['reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_9 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_10 (MaxPooling2D  (None, 1, 1, 100)   0           ['conv2d_10[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_11 (MaxPooling2D  (None, 1, 1, 100)   0           ['conv2d_11[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 3, 1, 100)    0           ['max_pooling2d_9[0][0]',        \n",
      "                                                                  'max_pooling2d_10[0][0]',       \n",
      "                                                                  'max_pooling2d_11[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 300)          0           ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 300)          0           ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 5)            1505        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,596,305\n",
      "Trainable params: 4,596,305\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_3 = Model(inputs=inputs_3, outputs=output_3)\n",
    "model_3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b01b746a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "796/796 [==============================] - 63s 78ms/step - loss: 1.8980 - accuracy: 0.3217 - val_loss: 1.8569 - val_accuracy: 0.4740\n",
      "Epoch 2/10\n",
      "796/796 [==============================] - 62s 78ms/step - loss: 1.8128 - accuracy: 0.4589 - val_loss: 1.8101 - val_accuracy: 0.4099\n",
      "Epoch 3/10\n",
      "796/796 [==============================] - 62s 78ms/step - loss: 1.7160 - accuracy: 0.5052 - val_loss: 1.7186 - val_accuracy: 0.5159\n",
      "Epoch 4/10\n",
      "796/796 [==============================] - 62s 78ms/step - loss: 1.6611 - accuracy: 0.5312 - val_loss: 1.6614 - val_accuracy: 0.5218\n",
      "Epoch 5/10\n",
      "796/796 [==============================] - 62s 78ms/step - loss: 1.6209 - accuracy: 0.5379 - val_loss: 1.6140 - val_accuracy: 0.5350\n",
      "Epoch 6/10\n",
      "796/796 [==============================] - 62s 78ms/step - loss: 1.5658 - accuracy: 0.5599 - val_loss: 1.6051 - val_accuracy: 0.5181\n",
      "Epoch 7/10\n",
      "796/796 [==============================] - 62s 78ms/step - loss: 1.5661 - accuracy: 0.5599 - val_loss: 1.5867 - val_accuracy: 0.5339\n",
      "Epoch 8/10\n",
      "796/796 [==============================] - 62s 78ms/step - loss: 1.5228 - accuracy: 0.5703 - val_loss: 1.7090 - val_accuracy: 0.5477\n",
      "Epoch 9/10\n",
      "796/796 [==============================] - 63s 79ms/step - loss: 1.5145 - accuracy: 0.5794 - val_loss: 1.7082 - val_accuracy: 0.5392\n",
      "Epoch 10/10\n",
      "796/796 [==============================] - 63s 79ms/step - loss: 1.5003 - accuracy: 0.5792 - val_loss: 1.6195 - val_accuracy: 0.5108\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "history_3 = model_3.fit(X_train, y_train, epochs=10, batch_size=batch_size, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe71e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_3.history['acc'])\n",
    "plt.plot(history_3.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_3.history['loss'])\n",
    "plt.plot(history_3.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34e1b85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 1s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[267, 315,  68,  31,   0],\n",
       "       [ 87, 297, 239,  62,   7],\n",
       "       [ 13,  85, 504, 106,   8],\n",
       "       [  5,  32, 196, 346, 132],\n",
       "       [  6,  13,  26, 335, 356]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_3 = model_3.predict(X_test)\n",
    "accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_3)))\n",
    "confusion_matrix(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b57807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 4: Again, but with binary classification (similar to SST2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "74f4a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SST2 is the same as SST1, but with neutrals removed and binary labels\n",
    "sst2_data = pd.concat([df_0.head(sample_size), df_1.head(sample_size), df_3.head(sample_size), df_4.head(sample_size)]).sample(frac=1)\n",
    "\n",
    "def merge_sentiments(x):\n",
    "    if x == 0 or x == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "sst2_data['Sentiment'] = sst2_data['Sentiment'].apply(merge_sentiments)\n",
    "\n",
    "sst2_tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>')\n",
    "sst2_tokenizer.fit_on_texts(sst2_data['Phrase'].values)\n",
    "\n",
    "sst2_X = sst2_tokenizer.texts_to_sequences(sst2_data['Phrase'].values)\n",
    "sst2_X = pad_sequences(sst2_X, sequence_length)\n",
    "\n",
    "sst2_y = sst2_data['Sentiment'].values\n",
    "\n",
    "sst2_X_train, sst2_X_test, sst2_y_train, sst2_y_test = train_test_split(sst2_X, sst2_y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "451e4bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_4 = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding_layer_4 = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=sequence_length,\n",
    "                            trainable=True)(inputs_4)\n",
    "\n",
    "reshape_4 = Reshape((sequence_length, embedding_dim, 1))(embedding_layer_4)\n",
    "\n",
    "conv_0_4 = Conv2D(num_filters, kernel_size=(3, embedding_dim), padding='valid', kernel_initializer='normal', activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_4)\n",
    "conv_1_4 = Conv2D(num_filters, kernel_size=(4, embedding_dim), padding='valid', kernel_initializer='normal', activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_4)\n",
    "conv_2_4 = Conv2D(num_filters, kernel_size=(5, embedding_dim), padding='valid', kernel_initializer='normal', activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_4)\n",
    "\n",
    "maxpool_0_4 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_4)\n",
    "maxpool_1_4 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_4)\n",
    "maxpool_2_4 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_4)\n",
    "\n",
    "concatenated_tensor_4 = Concatenate(axis=1)([maxpool_0_4, maxpool_1_4, maxpool_2_4])\n",
    "flatten_4 = Flatten()(concatenated_tensor_4)\n",
    "\n",
    "dropout_4 = Dropout(0.5)(flatten_4)\n",
    "# note the different activation\n",
    "output_4 = Dense(units=1, activation='sigmoid')(dropout_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "54b87fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 52)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, 52, 300)      4234500     ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)            (None, 52, 300, 1)   0           ['embedding_5[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 50, 1, 100)   90100       ['reshape_5[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 49, 1, 100)   120100      ['reshape_5[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 48, 1, 100)   150100      ['reshape_5[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_12 (MaxPooling2D  (None, 1, 1, 100)   0           ['conv2d_12[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_13 (MaxPooling2D  (None, 1, 1, 100)   0           ['conv2d_13[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_14 (MaxPooling2D  (None, 1, 1, 100)   0           ['conv2d_14[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 3, 1, 100)    0           ['max_pooling2d_12[0][0]',       \n",
      "                                                                  'max_pooling2d_13[0][0]',       \n",
      "                                                                  'max_pooling2d_14[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 300)          0           ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 300)          0           ['flatten_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            301         ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,595,101\n",
      "Trainable params: 4,595,101\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_4 = Model(inputs=inputs_4, outputs=output_4)\n",
    "\n",
    "# note we're using binary_crossentropy here instead of categorical\n",
    "model_4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5680e282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "637/637 [==============================] - 74s 115ms/step - loss: 130.5147 - accuracy: 0.5086 - val_loss: 0.6971 - val_accuracy: 0.4980\n",
      "Epoch 2/10\n",
      "637/637 [==============================] - 81s 127ms/step - loss: 0.6987 - accuracy: 0.5030 - val_loss: 0.6973 - val_accuracy: 0.5024\n",
      "Epoch 3/10\n",
      "637/637 [==============================] - 81s 127ms/step - loss: 0.6979 - accuracy: 0.5018 - val_loss: 0.6973 - val_accuracy: 0.4976\n",
      "Epoch 4/10\n",
      "637/637 [==============================] - 81s 126ms/step - loss: 0.6969 - accuracy: 0.4922 - val_loss: 0.6956 - val_accuracy: 0.5031\n",
      "Epoch 5/10\n",
      "637/637 [==============================] - 80s 126ms/step - loss: 0.6956 - accuracy: 0.4991 - val_loss: 0.6949 - val_accuracy: 0.5024\n",
      "Epoch 6/10\n",
      "637/637 [==============================] - 80s 125ms/step - loss: 0.6946 - accuracy: 0.5021 - val_loss: 0.6941 - val_accuracy: 0.5024\n",
      "Epoch 7/10\n",
      "637/637 [==============================] - 80s 126ms/step - loss: 0.6940 - accuracy: 0.4971 - val_loss: 0.6938 - val_accuracy: 0.5024\n",
      "Epoch 8/10\n",
      "637/637 [==============================] - 81s 126ms/step - loss: 0.6940 - accuracy: 0.4961 - val_loss: 0.6936 - val_accuracy: 0.5024\n",
      "Epoch 9/10\n",
      "637/637 [==============================] - 80s 126ms/step - loss: 0.6936 - accuracy: 0.5027 - val_loss: 0.6938 - val_accuracy: 0.4976\n",
      "Epoch 10/10\n",
      "637/637 [==============================] - 79s 124ms/step - loss: 0.6936 - accuracy: 0.4966 - val_loss: 0.6935 - val_accuracy: 0.5024\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "history_4 = model_4.fit(sst2_X_train, sst2_y_train, epochs=10, batch_size=batch_size, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be139404",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_4.history['acc'])\n",
    "plt.plot(history_4.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_4.history['loss'])\n",
    "plt.plot(history_4.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc1380cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 2s 17ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1400,    0],\n",
       "       [1429,    0]], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_4 = model_4.predict(sst2_X_test)\n",
    "\n",
    "accuracy_score(sst2_y_test, list(map(lambda v: v > 0.5, y_hat_4)))\n",
    "confusion_matrix(sst2_y_test, list(map(lambda v: v > 0.5, y_hat_4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ab5e6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN random       : 0.372737556561086\n",
      "CNN static       : 0.43665158371040724\n",
      "CNN trainable    : 0.5005656108597285\n",
      "Binary trainable : 0.4948745139625309\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN random       : \" + str(accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))))\n",
    "print(\"CNN static       : \" + str(accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_2)))))\n",
    "print(\"CNN trainable    : \" + str(accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_3)))))\n",
    "print(\"Binary trainable : \" + str(accuracy_score(sst2_y_test, list(map(lambda v: v > 0.5, y_hat_4)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecef9dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa8d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2d0229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4ff96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb0714f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae988ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c6ec8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efee8f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c010509c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6922baf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18790df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a44c2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e4c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081ef9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c31e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419708ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3512325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac5f89f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efe983d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683989b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
